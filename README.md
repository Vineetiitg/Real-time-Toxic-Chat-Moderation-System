

# Real-Time Chat Moderation Application (safechat)

SafeChat is a lightweight, fast, and AI-powered real-time chat moderation system designed to detect and prevent toxic, abusive, and unsafe messages before they appear in conversations. Instead of punishment, SafeChat promotes polite guidance, helping users communicate better while maintaining a safe and professional environment.

---
#  Toxic Chat Moderation System

ğŸš§ **Project Status: ---Active Development--- (Hackathon Prototype)** ğŸš§

This repository contains an evolving prototype developed as part of a hackathon.
Core architecture and model pipelines are Demonstrated.

BiLSTM & RoBERTa_base model is implemented, trained and tested on a demo (small) dataset, respective results are updated on the repository fully trained Model demonstration at the end of the project.

Remaining components will be finalized before the live prototype demonstration.

---

## Development Roadmap

### âœ… Completed
- Dataset ingestion & EDA
- RoBERTa Lora fine-tuning pipeline
- Muril Lora fine-tuning
- Evaluation metrics (F1, Precision, Recall)
- Project structure & configs

### ğŸš§ In Progress
- Ensemble inference logic
- FastAPI integration
- Real-time moderation flow

### â³ Planned (Before Demo Day)
- Live chat moderation UI
- Model optimization & pruning
- Final performance benchmarking


## Problem vs SafeChat Solution

| ğŸ”´ Problem | ğŸŸ¢ SafeChat Solution |
|-----------|---------------------|
| Slow manual moderation | Instant AI moderation |
| Keyword-based filtering | Context-aware AI |
| Punishment-based actions | Polite guidance |
| High false positives | Ensemble AI (High accuracy, Less false positives) |

## âš¡Key Features

- Real-time message analysis

- Context-aware AI moderation

- Guidance-based approach (not punishment)

- Toxicity, abuse, and unsafe content detection

- Polite alternative message suggestions

- Lightweight, fast, and scalable architecture

## ğŸ§  How It Works

1. User sends a message

2. SafeChat analyzes it instantly

3. If safe â†’ message is posted

4. If Toxic/harmful â†’ message is blocked/hidden

5. User receives a polite alternative suggestion

## âš™ï¸ Our Technology Stack

- NLP & Machine Learning

- RoBERTa with LoRa PEFT (context understanding)

- BiLSTM (sequence and sentiment learning)

- Ensemble decision Model (stacking multiple models)

- BART (Polite & Context aware Rephrasing)

- Rule-based filters (edge cases & safety)

- Backend: Python / SpringBoot

- Real-Time Processing: WebSockets / APIs
 
- Deployment: Cloud-ready & scalable

## ğŸš€ Technical Flow

  <img width="2816" height="1536" alt="Image" src="https://github.com/user-attachments/assets/a9a95bdb-d6cf-43b0-bc4b-16163b2cba3f" />

## ğŸ¯ Use Cases

- Social media platforms

- Online classrooms

- Team collaboration tools

- Gaming chats

- Community forums

## ğŸŒ± Benefits

- Safer digital environments

- Better user communication habits

- Higher user trust and retention

- Reduced moderation costs

- Professional and inclusive conversations

---
<div align='centre'>
  <h2>ğŸ“Œ Conclusion</h2>

SafeChat transforms moderation from punishment to guidance, ensuring real-time safety without disrupting conversation flow. It empowers platforms to build healthier, more respectful online communities.

</div>



## Directory Structure

- `app/`: Core application code.
  - `api.py`: FastAPI backend entry point.
  - `engine.py`: Logic for toxicity detection and rewriting.
  - `dashboard.py`: Streamlit/Gradio dashboard.
- `models/`: Pre-trained and fine-tuned model weights.
- `scripts/`: Training scripts (`bartTraining.py`, `ensembleTraining.py`).
- `tests/`: Unit and integration tests.

## Setup

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Run the API:
   ```bash
   python -m app.api
   ```
   or
   ```bash
   uvicorn app.api:app --reload
   ```

3. Run the Dashboard:
   ```bash
   python app/dashboard.py
   ```

## API Endpoints

- `POST /moderate`: Analyze text for toxicity.
- `GET /`: Health check.
